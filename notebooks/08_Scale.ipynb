{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Scale Your ML (DL) Prototype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tLike any other time-series problem, the volume of data is not that large for this project. \n",
    "\n",
    "•\tIn order to achieve scaling, I have separated the project into 3 separate parts:\n",
    "<ul>\n",
    "    <li>Collector – Collect stock historical price raw data from Finance API</li>\n",
    "    <li>Analyzer – Transform and Data analysis</li>\n",
    "    <li>Flask Web App – Web app to display the outcome of analysis</li>\n",
    "</ul>\n",
    "\n",
    "•\tAll the 3 modules are connected by a common Database table which controls the sequential execution of various stages of stock analysis for each stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\t<mark>Raw data collection</mark> using Finance API takes about <font color=\"red\">90 min</font> for collecting the entire data for all the stocks. Since this is a one-time activity per year, don’t see the need for scaling any further.\n",
    "\n",
    "•\tHowever, scaling can be achieved by running multiple instance of collector in case faster execution is required since the process is entirely managed via DB status "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tData clean-up, transformation and aggregation took about 1 hour for all the 1600+ stocks.\n",
    "\n",
    "•\tI have kept the start date as <code style=\"background:yellow\">1st Jan 1990</code> to collect stock data. However, the <code style=\"background:yellow\">earliest date for which data is available is – 2nd Jan 1991</code>, for TATA MOTORS. Since I am creating a separate model for each stock, max. rows per stock = 30 years * approx. 250 working days per year i.e. <code style=\"background:yellow\">maximum 7500 records per stock</code>.\n",
    "\n",
    "•\tHowever, the real challenge lies in the fact that I am running about 11 algorithms for each stock + 4 seasonal decompose methods. Also, I am doing past <code style=\"background:yellow\">data analysis for up-to 5 years + prediction for current year</code>. Thus, complete time series analysis for each stock takes about 30 to 45 minutes. Thus, for 1600 stocks it will take about 1000 hrs – i.e. about 40 days. Since, I have 2 laptops, I divided the load to finish the analysis in <code style=\"background:yellow\">20 days</code>. This being a one-time activity per year, don’t see the need for scaling any further. However, if required, the load can be divided further since the execution is entirely managed as a workflow from DB.\n",
    "\n",
    "•\tIn terms of individual algorithm performance, <code style=\"background:yellow\">Auto-ARIMA takes the max. time (about 2 to 5 minutes per model)</code> since it tries to fit different models to come out with the best possible parameters for the Model. \n",
    "\n",
    "•\tI have also tried to implement Seasonal ARIMA but due to significant size of lag i.e. 250 for yearly seasonality, it never executed even after running for 30+ hours. Hence, as per Auto-ARIMA documentation, I have used Fourier Featurizer to handle the seasonality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References: \n",
    "<ul>\n",
    "<li><a href=\"https://robjhyndman.com/hyndsight/longseasonality/\">Forecasting with long seasonal periods</a></li>\n",
    "<li><a href=\"https://alkaline-ml.com/pmdarima/modules/generated/pmdarima.preprocessing.FourierFeaturizer.html\">pmdarima.preprocessing.FourierFeaturizer</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flask Web App"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "•\tThe Web App provide an UI interface to view the outcome of seasonality analysis for each NSE stock. Analysis data is fetched from DB and the image + data files are fetched from shared locations which can be accessed from App server.\n",
    "\n",
    "•\tData is fetched for one stock at a time as selected by user.\n",
    "\n",
    "•\tThe app can be hosted on any cloud instance – I have used AWS Elastic Beanstalk. Data and image files can be shared on any storage accessible from App server."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
